---
title: "MÉTÉO"
author: "Anthony LEZIN"
date: "7/18/2020"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
library(knitr)
```

## I. La température (modèle explicatif)
<br>
<br>
Je dispose d'un certain nombre de relevés de diverses observations (température, pression, nébulosité, vent, etc..). L'objectif de cette partie est d'expliquer la vriable d'intérêt $température\ du\ lendemain$ en fonction des observations.


Quelques informations potentiellement utiles sur Bâle pour mieuxx appréhender les données (Hauteur : Bâle : 	268 m Min. 244 m Max. 363 m).
<br>
<br>

### A. Préparation des données
<br>
<br>
```{r}
met0=read.csv("/Users/anthonylezin/Desktop/projets_stat/Enoncés_Projets_Stats/Projet GLM/meteo.train.csv",header=TRUE,sep=",")
```


Après avoir visualisé sommairement les données, je décide de renommer les variables.

```{r}
nom=data.frame("X","Year","Month","Day","Hour","Minute","Tmoy2","Hmoy2","Prmoy","Plmoy","Snow","TNebmoy","HNebmoy","MNebmoy","LNebmoy","Sun","Ray","Wsmoy10","Wdmoy10","Wsmoy80","Wdmoy80","Wsmoy900","Wdmoy900","Wgmoy","Tmax2","Tmin2","Hmax2","Hmin2","Prmax","Prmin","TNebmax","TNebmin","HNebmax","HNebmin","MNebmax","MNebmin","LNebmax","LNebmin","Wsmax10","Wsmin10","Wsmax80","Wsmin80","Wsmax900","Wsmin900","Wgmax","Wgmin","pluie.demain","temp.demain")

#suppression des colonnes inutiles
met1=met0[,-c(1,5,6)]
nom1=nom[-c(1,5,6)]

#équivalences entre anciens noms et nouveaux noms
equivalences = cbind(names(met1),t(nom1))
rownames(equivalences)=c()
```

voici le tableau des équivalences pour les noms des variables :

```{r}
kable(equivalences)
```


Je renomme les variables dans un nouveau tableau et réarrange les colonnes par catégorie.


```{r}
#je renomme les varaiables
met2=met1
names(met2)=equivalences[,2]
#je réarrange l'ordre des variables pour regrouper celles de même catégorie
met3 <- met2[, c(1:4,23,22,5,25,24,6,27,26,7:9,29,28,10,31,30,11,33,32,12,35,34,13:15,37,36,16,17,39,38,18,19,41,40,20,21,43,42,44,45)]
```



### B. Premières visualisation de quelques dépendances potentielles

```{r,fig.width=9, fig.height=4.5}
par(mfrow=c(1,2))
plot(met3$temp.demain~met3$Prmax)
plot(met3$temp.demain~met3$Month)
```


La covariable $temp.demain$ semble liée (non linéairement) à la pression maximale de la veille. En effet, j'observe que la température est :

 * croissante pour une pression allant de $1000 mB$ à $1020 mB$
 * décroissante pour une pression allant de $1020 mB$ à $1030 mB$
 

Le second graphique montre que $temp.demain$ varie également selon les mois de l'année. Il y a un effet "saisonnalité". 



```{r,fig.width=9, fig.height=4.5}
par(mfrow=c(1,2))
boxplot(met3$temp.demain~met3$pluie.demain)
plot(met3$temp.demain~met3$Tmoy2)
```



Le fait de prévoir de la pluie le lendemain semble peu influer sur $temp.demain$.

En revanche, la température moyenne du jour semble linéairement très influente sur la covaraiable $temp.demain$, car le nuage de points est resséré autour d'une droite.

Je le vérifie sur une régression locale :


```{r,fig.width=9, fig.height=4.5}
plot(lm(temp.demain~Tmoy2, data=met3),which=1)
```

La courbe est très proche de la droite d'abscisse 0 et la répartition des résidus de part et d'autre de la courbe est excellente.


Je cherche désormais à visualiser une éventuelle relation entre la covariable $temp.demain$ et la vitesse du vent (à 10m).


```{r,fig.width=9, fig.height=4.5}
par(mfrow=c(1,2))
plot(met3$temp.demain~met3$Wsmax10)
# un "essai exotique"
plot(met3$temp.demain~sqrt(met3$Wsmax10))
```

la transformation avec $"\sqrt{}"$ entraine un "étalement du nuage" qui pourrait être intéressant.

### C. Selection d'un premier modèle

#### 1. Le modèle global

```{r}
temp=lm(temp.demain~.,data=met3)
summary(temp)
AIC(temp)
```


#### 2. Interprétation des coefficients

 * le coefficient de $Tmoy2$ est positif ("0.971") et fortement significatif.
On retrouve bien le fait que $temp.demain$ et $Tmoy2$ sont "fortement liées".
Plus précisemment, quand la température moyenne augmente d'une unité (donc de 1°C), la température du lendemain augmente de $0.971$ °C.

 * c'est en revanche, le phénomène inverse pour $Wsmin900$ puisque, cette fois-ci, le coefficient est négatif ("-0.0418").
Plus précisemment, quand la la vitesse minimale du vent (à une pression de 900 mB) augmente d'une unité, la température du lendemain diminue de $-0.0418$ °C.

 * C'est un peu différent pour la covariable $pluie.demain$ de part son caractère qualitatif et ses différentes modaltés ("FALSE, TRUE").
Le coefficient de "pluie.demainTRUE" est un **effet différentiel** par rapport à la cellule de référence "pluie.demainFALSE" **représentée par l'intercept** 
(il suffit d'enlever l'intercept du modèle pour vérifier la concordance des coefficients).

Ainsi, prévoir de la pluie demain fait baisser la température du lendemain de 0.462 °C (par rapport au fait de pévoir une absence de pluie).


#### 3. Un algorithme de sélection de modèle

Beaucoup de variables ne sont pas significatives.
J'élimine les moins pertinentes de manière progressive en me "laissant de la marge".


Pour cela, j'effectue la procédure suivante :

 * j'ôte que les covaraiables du modèle dont **la p-valeur est supérieure à 0,20**. 
 * j'effectue une régression linéaire sur les covariables restantes
 
 
J'obtiens successivement les modèles $temp1$, $temp2$, $temp3$ (je synthétiserai  les résultats essentiels dans un tableau).


```{r}
temp1=lm(temp.demain~ Year+Month+Day+Tmoy2+Tmin2+Tmax2+Hmax2+Prmoy+Plmoy+HNebmax+LNebmoy+Sun+Ray+Wsmax10+Wsmoy900
+Wsmin900+Wdmoy900+pluie.demain,data=met3)

temp2=lm(temp.demain~Month+Day+Tmoy2+Tmin2+Prmoy+Plmoy+HNebmax+LNebmoy+Ray+Wsmax10+Wsmoy900
+Wsmin900+Wdmoy900+pluie.demain,data=met3)

temp3=lm(temp.demain~Month+Day+Tmoy2+Tmin2+Prmoy+Plmoy+LNebmoy+Ray+Wsmax10+Wsmoy900+Wsmin900
+Wdmoy900+ pluie.demain,data=met3)
```


Il n'y a maintenant plus de covariables vérifiant mon critère de suppression.

Je l'adapte en supprimant désormais celles de **p-valeur supérieure à 0.10**.


```{r}
temp4=lm(temp.demain~Month+Day+Tmoy2+Tmin2+Prmoy+Plmoy+LNebmoy+Ray+Wsmax10+Wsmoy900+Wdmoy900
+ pluie.demain,data=met3)
```



Seule la covariable $Day$ est à la limite de la significativité. Je choisis de la conserver pour l'instant.
Le test global de Fisher fournit une p-valeur très proche de $0$ rejetant logiquement l'hypothèse de l'inutilité de la régression (et ce quelque soit les modèles testés).


Pour résumer l'étude faite précedemment, voici un tableau contenant les covariables intervenant dans chacun des modèles ainsi que leurs p-valeurs respectives :


covariables |$temp1$|$temp2$|$temp3$|$temp4$|
------------|-------|-------|----- |-------|
(Intercept)|6.52e-02|1.95e-03|1.36e-03|2.33e-03	|
Year|	1.86e-01	| | | |
Month	|1.52e-04	|7.35e-04|6.08e-04|5.90e-04	|	
Day	|8.37e-02	|7.42e-02	|7.08e-02	|6.61e-02	|
Tmoy2	|1.28e-19	|6.07e-96	|6.14e-97	|4.85e-100|
Tmin2	|4.63e-05	|3.64e-09 |4.19e-09	|6.18e-10	|
Tmax2	|2.63e-01	| | | |
Hmax2	|5.85e-01	| | | |
Prmoy	|2.46e-03	|2.45e-03	|1.76e-03	|2.88e-03	|
Plmoy	|6.43e-03 |1.44e-02	|1.56e-02	|1.00e-02	|
HNebmax	|2.12e-01	|5.31e-01	| | |
LNebmoy	|1.62e-01	|7.41e-03	|6.79e-03	|7.69e-03	|
Sun	|1.29e-01	| | | |
Ray	|1.77e-10	|9.77e-11	|6.84e-12	|2.69e-11	|
Wsmax10	|3.28e-10	|8.28e-10	|7.12e-10	|1.75e-09	|
Wsmoy900	|3.52e-03	|3.24e-03	|3.63e-03	|1.33e-02	|
Wsmin900	|7.53e-02	|6.34e-02	|7.07e-02	| |
Wdmoy900	|3.00e-03	|4.39e-03	|4.88e-03	|4.52e-03	|
pluie.demainTRUE	|1.65e-04	|3.27e-04 |1.71e-04	|2.35e-04|


Le modèle $temp4$ commence à être satisfaisant. J'étudie maintenant la structure de ses résidus.

\newpage

### D. Structure des résidus du modèle $temp4$

```{r,fig.width=9, fig.height=6}
par(mfrow=c(2,2))
plot(temp4)
```

* l'hypothèse **de linéarité du modèle** (“residuals vs fitted plot”) est acceptable. En effet, lorsque les réponses prédites par le modèle (fitted values) augmentent, les résidus restent globalement uniformément distribués de part et d’autre de 0. Cela montre, qu’en moyenne, la droite de régression est bien adaptée aux données.

 * le **QQ-plot** traduit une normalité qui est assez bien respectée (sauf en quelques "points leviers")

 * l'hypothèse **d'homogénéité des résidus** (“scale location”) est acceptée. En effet, la courbe rouge (courbe de regression locale) est globalement plate. Les résidus ont tendance à être répartis de façon homogène tout le long du gradient des valeurs prédites de $temp.demain$.

 * pour les **distances de Cook**, les données $682$, $1089$ $1099$ semblent être des points levier. Néanmoins, les distances sont toutes inférieures à $0.5$.
L'influence de ces valeurs sur les paramètres du modèle n’est pas vraiment problématique.
 

La structure générale des résidus m'indique que le modèle de régression linéaire $temp4$ est légitime.
 


### E. Amélioration potentielle du modèle sans interaction

Que se passe-t-il si j'enlève la covariable $Day$ ? (pour rappel, elle était à la limite de la significativité)

```{r}
temp5=lm(temp.demain~Month+Tmoy2+Tmin2+Prmoy+Plmoy+LNebmoy+Ray+Wsmax10+Wsmoy900+Wdmoy900+ pluie.demain,data=met3)
```


 * toutes les covariables présentes sont significatives
 * ce modèle contient 11 covariables 
 * la structure des résidus est sensiblement identique au modèle précédent


##### Comparaison des modèles précédents en fonction de $R_{adj}^2$ et de $AIC$


```{r}
R_AIC=function(A){
  return(c(as.numeric(summary(A)$"adj.r.squared"),AIC(A)))
}

temp_list=paste('temp',1:5,sep='')
R_tab=NULL
for (i in 1:5){
R_tab=t(c(R_tab,R_AIC(get(temp_list[i]))))
}

R_tab=t(matrix(R_tab,ncol=5,nrow=2,byrow = F))
rownames(R_tab)=temp_list
colnames(R_tab)=c("R^2_adj","AIC")
kable(R_tab)
```


Au vu de ces 2 critères, **je choisis $temp4$** comme **premier modèle de référence** (sans interaction). En effet,

 * l'$AIC$ est parmi les plus basses
 * le nombre de covariables est réduit
 * le $R_{adj}^2$ $(0.930)$ est sensiblement identique à tous les modèles


J'ai pris le le parti de ne pas privilégier son concurrent direct $(temp5)$, car la covariable enlevée est à la limite de la significativité (covariable que je souhaite conserver pour l'instant).



### F. Procédure automatique de selection de modèle


Voici les codes respectifs pour les méthodes ascendantes, descendantes et Stepwise.

```{r, eval=FALSE}
library(MASS)
modselect_f=stepAIC(temp,temp.demain~.,data=
met3,trace=TRUE,direction=c("forward"))
summary(modselect_f)
AIC(modselect_f)
```


```{r, eval=FALSE}
modselect_f=stepAIC(temp,temp.demain~.,data=
met3,trace=FALSE,direction=c("backward"))
summary(modselect_f)
AIC(modselect_f)
```


```{r, eval=FALSE}
modselect_f=stepAIC(temp,temp.demain~.,data=
met3,trace=FALSE,direction=c("both"))
summary(modselect_f)
AIC(modselect_f)
```


Les méthodes descendantes et stepwise fournissent une AIC un peu meilleure que le modèle de référence $temp4$ ($AIC=4820$) mais utilise 22 covariables (dont un certain nombre non signicatives).

Je ne retiens pas le modèle obtenu et conserve le modèle $temp4$.


### G. Procédure automatique de recherche d'un modèle plus petit que $temp4$


```{r, fig.width=9, fig.height=9}
library(leaps)
par(mfrow=c(2,2))
recherche.ex=regsubsets(temp.demain~

                          Month+Day+Tmoy2+Tmin2+Prmoy+Plmoy+LNebmoy+Ray+Wsmax10+Wsmoy900+Wdmoy900+ pluie.demain,
  int=T,nbest=1,nvmax=13,method="exhaustive",data=met3)


l=c("bic","Cp","adjr2","r2")
for (i in 1:4){
plot(recherche.ex,scale=l[i])
}
```


La recherche de modèles plus petits par procédure automatique conforte mes choix pour 3 critères sur 4  :

 * le $R^2$ (évidemment, car il augmente "mécaniquement" avec un nombre croissant de covariables)
 * le $R_{adj}^2$ (déjà vérifié plus haut)
 * le $C_p$ de Mallows


Je retiens également le meilleur modèle fourni pour le critère du BIC (que j'appelle $temp6$) afin de tester ultérieurement ses qualités prédictives.

```{r}
temp6=lm(temp.demain~Month+Tmoy2+Tmin2+Prmoy+Plmoy+LNebmoy+Ray+Wsmax10+Wdmoy900+ pluie.demain,data=met3)
```


### H. Ajout des interactions


Je commence par effectuer un "scatterplot" pour y déceler d'eventuelles informations sur des corrélations potentielles.

Les corrélations élevées m'indiquent des pistes dans la recherche des interactions potentielles.

```{r}
# préparation des données
met4 <- met3[,c(2:5,10,13,24,28,31,37,40,44)]

# Scatterplot

# Histogramme sur la diagonale
panel.hist=function(x,...){
  usr=par("usr"); on.exit(par(usr))
  par(usr=c(usr[1:2], 0, 1.5) )
  h=hist(x, plot = FALSE, col="lightblue")
  breaks=h$breaks; nb = length(breaks)
  y=h$counts ; y<-y/max(y)
  rect(breaks[nb], 0, breaks[-1], y , col="cyan", ...)
}  
# coefficients de correlation en valeur absolue sur la partie haute
# Taille de police proportionnelle au coefficient de corrélation
  
panel.cor = function(x,y,digits=2,prefix="",cex.cor,...){
  usr=par("usr"); on.exit(par(usr))
   par(usr=c(0,1,0,1 ))
   r=abs(cor(x,y))
   txt=format(c(r,0.123456789),digits=digits)[1]
   txt=paste(prefix, txt, sep="")
   if(missing(cex.cor)) cex.cor=0.8/strwidth(txt)
   text(0.5, 0.5, txt, cex=cex.cor*r)
}

# regression linéaire lm sur la partie basse ;
# d'où le signe du coefficient de corrélation.
  
  panel.lm = function(x,y)
  {
  points(x,y)
  abline(lm(y~x))
  lines(lowess(y,x),col="red")
 }  
# on regroupe le tout
pairs(met4,diag.panel=panel.hist,cex.labels=2,font.labels = 2,
      upper.panel=panel.cor, lower.panel=panel.lm)
```


La figure est très chargée et difficlement lisible (de part le nombre important de covariables), néanmoins, quelques informations en ressort.

Il semble y avoir une corrélation importante en $Tmoy2$ et $Tmin$ $(r=0.97)$ d'une part, $Tmoy2$ et $Ray$ $(r=0.70)$ d'autre part.
J'étudie l'interaction entre ces covariables dans un nouveau modèle.

J'y ajoute également l'interaction entre $Tmoy2$ et $Wsmax10$, car j'ai l'intuition que la température moyenne dépend fortement de la vitesse du vent.


```{r}
temp4b=lm(temp.demain~Month+Day+Tmoy2*(Tmin2+Wsmax10+Ray)+Prmoy+Plmoy+LNebmoy+Wsmoy900+Wdmoy900
+ pluie.demain,data=met3)

AIC(temp4b)
```

L'AIC baisse de un peu. Avant de supprimer les interactions et covariables non significatives, je teste l'interaction entre $Tmoy2$ et $Prmoy$ (intuition sur un lien potentiel "température" et "pression")


```{r}
temp4c=lm(temp.demain~Month+Day+Tmoy2*(Tmin2+Wsmax10+Ray+Prmoy)+Plmoy+LNebmoy+Wsmoy900+Wdmoy900
+ pluie.demain,data=met3)

AIC(temp4c)
```

L'ajout de toutes ces interactions semblent déstabiliser le modèle. Beaucoup de covariables et d'interactions dépassent allègrement le seuil de significativité.

l'intercept, ainsi que la pression moyenne s'avèrent désormais totalement inutiles.


##### - Un essai "brutal"-


Dans le modèle précédent, je décide d'enlever brutalement tout ce qui n'est pas significatif.


```{r}
temp4c2=lm(temp.demain~-1+Month+Tmoy2:Prmoy+Tmoy2+Tmin2+Tmoy2:Wsmax10+Plmoy+LNebmoy+Ray+Wsmoy900
+Wdmoy900+ pluie.demain,data=met3)
summary(temp4c2)
AIC(temp4c2)
```


Ce modèle ne contient que des éléments significatifs, mais est peut-être un peu petit en taille pour bien décrire la variable d'intérêt. l'AIC est somme toute convenable.

Je retiens ce modèle pour le tester ultérieurement.


Je décide de repartir du modèle $temp4c$ et d'enlever progressivement les éléments les moins significatifs :

 * l'interaction entre $Tmoy2$ et $Ray$ 
 * les covariable $Prmoy$, $Wsmoy900$ et $Wdmoy900$ (je ne conserve que l'interaction entre ces 2 dernières)
 


```{r}
temp4d=lm(temp.demain~Month+Day+Tmoy2:Prmoy+Tmoy2*(Tmin2+Wsmax10)+Plmoy+LNebmoy+Ray+Wsmoy900:Wdmoy900
+ pluie.demain,data=met3)

AIC(temp4d)
```

l'AIC (4803), tout comme le nombre de covariables continue de baisser.

Malgré la faible significativité, je conserve l'interaction entre $Tmoy2$ et $Tmin2$ (j'ai l'intuition que ces voriables sont quand même liées) et enlève les covariables à la limite de la significativité $Day$ et $Wsmax10$.


```{r}
temp4e=lm(temp.demain~Month+Tmoy2:Prmoy+Tmoy2*Tmin2+Tmoy2:Wsmax10+Plmoy+LNebmoy+Ray+Wsmoy900:Wdmoy900
+ pluie.demain,data=met3)

AIC(temp4e)
```


l'AIC remonte un peu (4806). N'ayant pas réussi à la rendre significative, je me résouds à finalement supprimer l'interaction entre $Tmoy2$ et $Tmin2$.

J'y ajoute "une touche exotique" car j'avais observé dans le scatterplot qu'il pourrait être intéressant de régresser $sqrt{Wsmax10}$ (au lieu de $Wsmax10$) au vu des régressions 2 à 2.


```{r}
temp4f=lm(temp.demain~Month+Tmoy2:(Prmoy+Wsmax10)+Tmoy2+Tmin2+Plmoy+LNebmoy+Ray+Wsmoy900:Wdmoy900
+sqrt(Wsmax10):LNebmoy+ pluie.demain,data=met3)

AIC(temp4f)
```

J'ai l'AIC la plus basse de tous mes essais (4801.6), un nombre réduit de covariables, d'interactions et tous les éléments sont très significatifs.

\newpage

J'étudie la structure des résidus afin de potentiellement valider le modèle.
Je décide de les comparer à ceux de $temp4$ dont j'ai déjà effectué l'étude précédemment.


```{r,fig.width=9, fig.height=7}
par(mfrow=c(2,2))
for (i in 1:4){
  plot(temp4f,i)
  plot(temp4,i)
}
```


Les modèles sont assez similaires. $temp4f$ (à gauche) semble même légèrement meilleur que $temp4$ (à droite) pour les éléments étudiés sur les 3 premiers graphiques (les interprétations ont été faites précédemment).

les distances de Cook traduisent un nombre plus important de points leviers sur le modèle $temp4f$, mais leur valeurs inférieures à 0.5 minimise leur influence sur le modèle.


#### Conclusion

Ce modèle me semble intéressant. Il explique relativement bien la variable à expliquer.

La synthèse de l'ensemble de l'étude m'enclint à privilégier les modèles $temp4c2$, $temp4d$, $temp4f$.

Je vais maintenant désormais tester leurs qualités prédictives, ainsi que toutes celles de leurs concurrents.



## II. La température (prédiction)

<br>
<br>

### A. Préparation des données
<br>
<br>
```{r}
# chargement des données test
metpr0=read.csv("/Users/anthonylezin/Desktop/projets_stat/Enoncés_Projets_Stats/Projet GLM/meteo.test.csv",header=TRUE,sep=",")
```



```{r}
# suppression des colonnes inutiles
metpr1=metpr0[,-c(1,5,6)]
metpr2=metpr1

names(metpr2)=equivalences[,2]

#réarrangement de l'ordre des variables afin de regrouper celles concernant la même catégorie
metpr2 <- metpr2[,c(1:4,23,22,5,25,24,6,27,26,7:9,29,28,10,31,30,11,33,32,12,35,34,13:15,37,36,16,17,39,38,18,19,41,40,20,21,43,42,44,45)]

# pour rappel, voici la liste des modèles :
mod=c("temp","temp1","temp2","temp3","temp4","temp5","temp6","temp4b","temp4c",
      "temp4c2","temp4d","temp4e","temp4f")
```


Je teste les qualités prédictives des modèles $temp4$, $temp4c2$, $temp4d$, $temp4f$ en affichant **l'intervalle de confiance associé** (au seuil de $95\%$) sur le fichier "test".

Par ailleurs, je les compare à celles du fichier original $temp$.


```{r}
# choix des modèles : "temp","temp4","temp4c2","temp4d","temp4f"
temp_list2=mod[c(1,5,10,11,13)]

amp=function(model){
  predict(model, newdata=metpr2,interval="confidence")
}


df1=data.frame(head(amp(temp)),head(amp(temp4)))
df2=data.frame(head(amp(temp4c2)),head(amp(temp4d)),head(amp(temp4f)))

model.names=function(model){
  A=c("fit(model)","lwr(model)","upr(model)")
  return(A)
}

colnames(df1)=c(model.names(temp),model.names(temp4))
colnames(df2)=c(model.names(temp4c2),model.names(temp4d),model.names(temp4f))
kable(df1)
kable(df2)
```

Le travail effectué ci-dessus répond donc à la question en fournissant une préduction continue de la température sur le fichier "test" selon mes meilleurs modèles. Pour en améliorer l'interprétation, je décide de poursuivre l'exploitation des résultats.



L'amplitude de l'intervalle de confiance dépend bien évidemment du modèle. Je calcule l'amplitude moyenne.


```{r}
# amplitude moyenne de l'intervalle de prédiction
amp_moy=function(model){
  mean(amp(model)[,3]-amp(model)[,2])
}
df4=NULL
for (i in 1:5){
df4[i]=amp_moy(get(temp_list2[i]))
}

df4=matrix(df4,ncol=5,nrow=1)
rownames(df4)="Amplitude moyenne"
colnames(df4)=temp_list2
df4
```


Comme attendu, c'est lorsque le modèle contient toutes les covariables que l'intervalle de prédiction est le plus large et donc le "plus permissif".

Pour un modèle possédant un nombre raisonnable de covariables, l'amplitude moyenne est comprise entre 0.7°C et 0.8°C.


### B. Mesure de la qualité de prédiction (partie 1)

#### 1. Une première étape

Les intervalles de confiance des modèles raisonnables sont "un peu étroits" pour évaluer convenablement la qualité de la prédiction. En effet, une différence de 0.5°C entre la valeur réelle et la valeur prédite classerait cette dernière comme une erreur de prédiction.

Voici une fonction permettant (entre autres) de vérifier ce fait.


```{r}
qual_pred.temp=function(model,j){
  A= predict(model, newdata=metpr2,interval="confidence")
  tmp=cbind(A[,2]-j , A[,3]+j)
  
B=NULL
  for (i in (1:dim(tmp)[1])){
    B[i]=(metpr2[i,45]>tmp[i,1]) && (metpr2[i,45]<tmp[i,2])
  }
  return(c(sum(B),100*mean(B)))
}
qual_pred.temp(temp4,0)
```


Voici le nombre de prédictions correctes (sur 290), ainsi que le taux de bonnes prédictions pour les modèles $temp, temp4, temp4c2, temp4d, temp4f$.


```{r}
df5=NULL
for (i in 1:5){
df5=t(c(df5,qual_pred.temp(get(temp_list2[i]),0)))
}

df5=t(matrix(df5,ncol=2,nrow=5,byrow = T))
colnames(df5)=temp_list2
rownames(df5)=c("bonnes prédictions","taux (en %)")
kable(df5)
```

Les résultats extrêmement décevants, néanmoins :

une différence de 1°C doit-elle obligatoirement être considérée comme une erreur de prédiction ?

Je propose une méthode "plus en accord" avec l'usage courant en fixant aribitrairement la règle suivante :


je considère alors comme une erreur de prédiction, une température observée n'appartenant pas à l'intervalle de confiance dont "j'élargis chacune des bornes" de j°C.

Le cas échéant, je considère la température comme convenablement prédite.

Voici une fonction iniquant les qualités prédictives des modèles précédents suivant ce critère.


```{r}
# renommage de la fonction
qpt=function(model){
  V=NULL
  for (i in (1:3)) {
    V[i]=qual_pred.temp(model,i-1)[2]
  }
  return(V)
}
df6=NULL
for (i in (1:length(mod))){
  df6=t(c(df6,(qpt(get(mod[i])))))
}
df6=matrix(df6,ncol=3,nrow=length(mod),byrow = T)
rownames(df6)=mod[1:length(mod)]
colnames(df6)=c("bornes +0°C", "bornes +1°C","bornes +2°C")
kable(df6)
```

#### 2. Résumé des informations et choix de modèle


J'ai choisi $temp4$ comme modèle de base, puis j'ai cherché à travailler sur les interactions entre les covaraiables explicatives afin :

 * d'en minimiser l'AIC
 * d'en maximiser le pouvoir prédictif.

Pour rappel, le modèle $temp4$ est une régression linéaire contenant les covariables suivantes :

$Month, Day, Tmoy2, Tmin2, Prmoy, Plmoy, LNebmoy, Ray, Wsmax10, Wsmoy900, Wdmoy900,  pluie.demain$


Voici un tableau résumant mes divers essais avec leurs caractéristiques.

modèles | caractéristsiques du modèle | nbre de cov. | AIC | $R_{adj}^2$ | qualité de prédiction (bornes +1°C)
----| ---------------------|----|--|--| ---------|
temp |toutes les covariables |45|4855 |0.929|71.7 %
temp4  |aucune interaction                 | 13| 4854 | 0.928| 59.3%
temp4b |(temp4) + Tmoy2*(Tmin2+Ray+Wsmax10)| 16| 4816 | 0.930 | 61.0%
temp4c |(temp4b) + Tmoy2:Prmoy             | 17| 4812 | 0.930| 61.4%
temp4d |(temp4c) + Wsmoy900:Wdmoy900 - (Prmoy + Tmoy2:Ray - Wdmoy900 - Wsmoy900)| 14| 4803| 0.931 | 59.3%
temp4e | (temp4d) - Day - Wsmax10          | 12 | 4806 | 0.930 | 60.7%
temp4f | (temp4e) - Tmoy2:Tmin2 + LNebmoy:sqrt(Wsmax10) | 12 | 4801.7 | 0.931 | 61% 
temp4c2 |(temp4c) - (Intercept + Wsmax10 + Day +Tmoy2:(Ray+Tmin2))| 12 | 4812 | 0.983 | 60.7%

Lorsque l'on augmente les marges des bornes de l'intervalle à 2°C, la qualité de prédiction des modèles (située entre $82.8\%$ et $86.6\%$) s'uniformise ayant pour conséquences de moins bien "séparer" les modèles. 

Il est néanmoins intéressant de remarquer que dans plus de $82\%$ des cas, la température observée se situe à moins de 2.4°C de la température prédite.


### C. Mesure de la qualité de prédiction (partie 2)


Dans cette partie je décide d'utiliser un autre critère pour mesurer la qualité prédictive du modèle.

En effet, il se peut très bien que : 

 * dans un modèle A, les données prédites soient 0.6°C supérieure ou inférieure à leurs valeurs observées contenues dans le jeu test les excluant systématiquement de l'intervalle de confiance initial et par voie de fait, les classant en une erreur de prédiction ("au niveau + 0°C")

 * dans un modèle B, certaines données soient dans l'intervalle, alors que d'autres sont distantes de plus de 5°C de la valeur observée.

 

La partie précédente m'aurait engagé à délaisser le modèle A au profit du modèle B, alors qu'intuivement, il est asssez proche de la réalité et a simplement le "mauvais goût de ne jamais tomber juste".

Afin de palier a ce problème, je vais désormais considérer **l'erreur moyenne de prédiction**

 * en valeur absolue (en calculant $\frac{\sum_{i=1}^{i=n}|y_i-\widehat{y_i}|}{n}$)
 * quadratique (au carré) en calculant $\frac{\sum_{i=1}^{i=n}(y_i-\widehat{y_i})^2}{n}$. Cela aura pour effet "d'accentuer" les erreur de prédiction éloignées.



```{r}
erp=function(model){
  A=mean(abs(amp(model)-metpr2[,45]))
  B=mean(abs(amp(model)-metpr2[,45])^2)
  return(c(A,B))
}

df7=data.frame(erp(temp),erp(temp4),erp(temp4c2),erp(temp4d),erp(temp4f))
rownames(df7)=c("erreur moyenne (en valeur absolue)","erreur quadratique moyenne")
colnames(df7)=temp_list2
kable(df7)
```

Avec ce critère, la qualité prédictive du modèle $temp$ est **la plus mauvaise**, confirmant ainsi le fait que le modèle idéal ne peut pas être celui qui contient l'ensemble des covariables.


Voici la synthèse de l'ensemble des résultats de l'étude des qualités prédictives des modèles.


modèles |nbre de cov. |$AIC$ |$R_{adj}^2$ |qualité (+1°C)| $\frac{\sum_{i=1}^{i=n}|y_i-\widehat{y_i}|}{n}$|$\frac{\sum_{i=1}^{i=n}(y_i-\widehat{y_i})^2}{n}$
-----| ---| ------| ----| -----| ------ |------ |
temp   | 45|4855 | 0.929|71.7 % |1.522 |4.073|
temp4  | 13|4854 | 0.928| 59.3%|1.465|3.674|
temp4b | 16|4816 | 0.930 | 61.0%|1.431|3.478|
temp4c | 17|4812 | 0.930| 61.4%| 1.413|3.413|
temp4d | 14|4803 | 0.931 | 59.3%|1.414|3.375|
temp4e | 12|4806 | 0.930 | 60.7%|1.404|3.378|
temp4f | 12|4801.7| 0.931 | 61%|1.419|3.402|
temp4c2| 12|4812 | 0.983 | 60.7%|1.407|3.414|


3 modèles ressortent du lot ($temp4e$, $temp4f$ et $temp4c2$). En effet, ces modèles possèdent 
 * un nombre minimal de covariables
 * une AIC assez basse (entre 4801.7 et 4812)
 * un bon pouvoir prédictif.

Parmi ces modèles, $temp4c2$ "tire son épingle du jeu" avec son **$R_{adj}^2$ 5% supérieur aux autres**. Il explique mieux la température du lendemain tout en étant  sensiblement au même niveau prédictif que ses concurrents.

Pour répondre à la dualité ("explication/prédiction"), je sélectionne donc modèle le modèle **$temp4c2$**.



### Un meilleur modèle ?

Je regarde si le nombre des covariables intervenant dans le modèle $temp4c2$ ne peut être réduit.

```{r,fig.width=9, fig.height=7}
par(mfrow=c(2,2))
recherche.ex2=regsubsets(temp.demain~-1+Month+Tmoy2:Prmoy+Tmoy2+Tmin2+Tmoy2:Wsmax10+Plmoy+LNebmoy
+Ray+Wsmoy900+Wdmoy900+ pluie.demain,
  int=T,nbest=1,nvmax=12,method="exhaustive",data=met3)

for (i in 1:4){
  plot(recherche.ex2,scale=l[i])
}
```

Dans tous les critères, le modèle semble meilleur si j'enlève l'interaction $Tmoy2:Prmoy$.

Je vais les comparer (au vu de l'AIC) et vérifier les qualités prédictives du nouveau modèle.



```{r}
# modèle temp4c2 privé de "Tmoy2:Prmoy".
temp4c3=lm(temp.demain~-1+Month+Tmoy2+Tmin2+Tmoy2:Wsmax10+Plmoy+LNebmoy+Ray
           +Wsmoy900+Wdmoy900+ pluie.demain,data=met3)
```


```{r, eval=FALSE}
AIC(temp4c3)
summary(temp4c3)$adj.r.squared
qpt(temp4c3)
erp(temp4c3)
```


modèles |nbre de cov. |$AIC$ |$R_{adj}^2$ |qualité (+1°C)| $\frac{\sum_{i=1}^{i=n}|y_i-\widehat{y_i}|}{n}$|$\frac{\sum_{i=1}^{i=n}(y_i-\widehat{y_i})^2}{n}$
--------| ---| ------| ----| -----| ------ |------ |
temp4c2| 12|4812 | 0.983 | 60.7%|1.407|3.414|
temp4c3| 11|4830 | 0.983 | 60.0%|1.430|3.510|


Au vu des résultats, je conserve le modèle $temp4c2$ malgré la covariable en moins. En effet, au long de cette étude, j'ai choisi le $AIC$ et le $R_{adj}^2$  comme critères de sélection.

Mon **modèle idéal** est donc le modèle **$temp4c2$**

Enfin, parce qu'il le faut le faire à un moment, je m'arrête ici...


Voici l'export des prédictions associées au modèle $temp4c2$.

```{r}
# Export des prédictions associées au modèle "temp4c2"

amp.temp4c2=round((amp(temp4c2)), digits=2)

amp.temp4c2=data.frame(cbind(round(amp(temp4c2)[,1],digits=2),
      data.frame(paste("[",amp.temp4c2[,2],":",amp.temp4c2[,3],"]",sep=""))))
colnames(amp.temp4c2)=c("temp. pred", "confiance (95%)")

kable(head(amp.temp4c2))
write.csv(amp.temp4c2, "amp.temp4c2.csv", row.names=FALSE, sep="t ",dec=".")
```


## III. La pluie


Dans cette partie, je vais tenter d'expliquer la variable $pluie.demain$ en fonction des données récoltées. C'est une variable binaire (avec la convention 1 = TRUE et 0 = FALSE).
Je vais donc effectuer une GLM pour proposer un modèle permettant à la fois d'expliquer les données, mais également de prédire s'il pleuvra ou non demain.


### A. Premières visualisation de quelques dépendances potentielles

Les couple à étudier étant du type quantitatif/qualitatif, j'utilise un "boxplot".


```{r, fig.width=9, fig.height=4.5}
par(mfrow=c(1,2))
boxplot(met3$Tmoy2~met3$pluie.demain)
boxplot(met3$Prmax~met3$pluie.demain)
```



Les différences sont minimes dans le 1er cas, on peut toutefois noter qu'il a plu lorsque la température de la veille était plus élevée en moyenne.

En revanche, il y a bien un effet Pression maximale sur la pluie le lendemain.
J'affinerai cette étude lors de l'étude entre les covariables sélectionnées.


Voici le modèle commplet :

```{r}
pluie0=glm(pluie.demain~.,family = binomial, data=met3)
summary(pluie0)
```


### B. Interprétation des coefficients


C'est désormais la "log côte anglaise des $p_i$" qui est directement interpératable (lorque le coefficient positif, la probabilité qu'il pleuve le lendemain augmente et réciproquement).
 
 * à titre d'interprétation, le coefficient de $Prmax$ est négatif ("-2.81e-01") et est fortement significatif.
On retrouve bien le fait que $temp.demain$ et $Prmax$ sont "fortement liées".
Plus précisemment, quand la pression maximale augmente d'une unité, la probabilité qu'il pleuve le lendemain diminue (car le coefficient est négatif) et est multipliée par $\frac{e^{-0.281}}{1+e^{-0.281}}$.

 * c'est en revanche le phénomène inverse pour la température moyenne, puisque, cette fois-ci, le coefficient est positif ("3.13e-01").


### C. Sélection d'un premier modèle

Beaucoup de covariables sont peu influentes. Il conviendra de les enlever du modèle.

on procède comme dans la partie I., on supprime progressivement les covariables de **p-valeur supérieure à 0.2**, puis on regarde l'évolution des coefficients restants.
<br>
<br>
```{r}
pluie1=glm(pluie.demain~Year + Month + Day+ Tmoy2 + Prmoy + Plmoy + Snow +Ray +Wsmoy80 +Wdmoy80 
           +Wdmoy900 +Tmin2 +Prmax +Prmin +TNebmin +MNebmax +Wsmax10 +Wsmin10 +Wsmin80 +Wsmax900 
           +Wgmax+temp.demain,
           ,family = binomial, data=met3)

pluie2=glm(pluie.demain~Year  + Day+ Tmoy2 + Prmoy + Plmoy + Snow +Wsmoy80 +Wdmoy80 +Wdmoy900 +Tmin2 
           +Prmax +Prmin +TNebmin +MNebmax +Wsmax10 +Wsmin10 +Wgmax +temp.demain,
             ,family = binomial, data=met3)

pluie3=glm(pluie.demain~Year+ Tmoy2 + Prmoy + Plmoy + Snow +Wsmoy80 +Wdmoy80 +Wdmoy900 +Tmin2 
           +Prmax +Prmin +TNebmin +MNebmax +Wsmax10 +Wsmin10 +Wgmax+ +temp.demain,
             ,family = binomial, data=met3)
```



L'intercept est toujours non significative, j'essaie un modèle alternatif en l'ôtant ici.


```{r}
pluie3b=glm(pluie.demain~-1+Year+ Tmoy2 + Prmoy + Plmoy + Snow +Wsmoy80 +Wdmoy80 +Wdmoy900 +Tmin2 
            +Prmax +Prmin +TNebmin +MNebmax +Wsmax10 +Wsmin10 +Wgmax +temp.demain,
             ,family = binomial, data=met3)
```


l'AIC a peu évolué. Je reprends mon procédé précédent, quitte à ré-enlever l'intercept plus tard.

```{r}
pluie4=glm(pluie.demain~Year+ Tmoy2 + Prmoy + Plmoy + Snow +Wsmoy80 +Wdmoy80 +Wdmoy900 +Prmax 
           +Prmin +TNebmin +MNebmax +Wsmax10 +Wsmin10 +Wgmax+temp.demain,
             ,family = binomial, data=met3)
```

Je considère ce modèle $pluie4$ comme "modèle de travail". Il contient les varaiables : $Year, Tmoy2, Prmoy, Plmoy, Snow, Wsmoy80, Wdmoy80, Wdmoy900, Prmax , Prmin, TNebmin, MNebmax, Wsmax10, Wsmin10, Wgma, temp.demain$. 

Ce modèle contient 16 covariables. J'enlève $Plmoy$ (comma variable non significative) et ajoute $Tmin$ (comme intuition).

```{r}
pluie4.1=glm(pluie.demain~Year+ Tmoy2 + Prmoy + Snow +Wsmoy80 +Wdmoy80 +Wdmoy900 +Tmin2 +Prmax 
             +Prmin +TNebmin +MNebmax +Wsmax10 +Wsmin10 +Wgmax+temp.demain,
             ,family = binomial, data=met3)

```


Cet essai s'avère infructueux. Je repas du modèle $pluie4$ et supprime simplement la covariable $Plmoy$. Ce modèle contient 15 covariables.


```{r}
pluie5=glm(pluie.demain~Year+ Tmoy2 + Prmoy  + Snow +Wsmoy80 +Wdmoy80 +Wdmoy900 +Prmax +Prmin 
           +TNebmin +MNebmax +Wsmax10 +Wsmin10 +Wgmax+temp.demain,
             ,family = binomial, data=met3)

# J'enlève la covariable "Snow". Ce modèle contient 14 covariables

pluie6=glm(pluie.demain~Year+ Tmoy2 + Prmoy +Wsmoy80 +Wdmoy80 +Wdmoy900 +Prmax +Prmin +TNebmin 
           +MNebmax +Wsmax10 +Wsmin10 +Wgmax+temp.demain,
             ,family = binomial, data=met3)

# L'intercept n'est toujours pas significative. Je l'ôte du modèle.

pluie7=glm(pluie.demain~ -1 +Year+ Tmoy2 + Prmoy +Wsmoy80 +Wdmoy80 +Wdmoy900 +Prmax +Prmin +TNebmin +MNebmax +Wsmax10 +Wsmin10 +Wgmax+temp.demain
             ,family = binomial, data=met3)
summary(pluie7)
```


Voici un tableau contenant les covariables intervenant dans chacun des modèles ainsi que leurs p-valeurs respectives intervenant dans les modèles étudiés :


covariables |$pluie4$|$pluie4.1$|$pluie5$|$pluie6$|$pluie7$|
------------|-------|-------|----- |-------|-------|
(Intercept)|2.8e-01|3.1e-01 |2.9e-01|2.3e-01| |
Year|	2.7e-02	|3.2e-02 |2.7e-02|1.8e-02 |9.4e-11
Tmoy2	|4.4e-05	|1.3e-04|4.0e-05|3.2e-05|3.8e-05
Prmoy	|1.9e-04	|2.9e-04|3.2e-04|2.4e-04|2.0e-04
Plmoy	|1.7e-01  |    	|   	| 	| 	|
Tmin2	|       	|5.3e-02 |    	|   	|   	|
Snow	|8.4e-02 |1.2e-01|1.2e-01	|     |
Wsmoy80	|2.0e-04|2.3e-04|2.4e-04|2.6e-04|2.8e-04
Wdmoy80	|2.1e-02|9.5e-02|2.8e-02|2.2e-02|2.4e-02 
Wdmoy900	|8.9e-04|1.9e-04|4.9e-04|3.5e-04|3.5e-04|
Prmax	|4.1e-04	|4.4e-04  |6.6e-04|2.9e-04|2.5e-04
Prmin	|1.4e-05|3.3e-05	  |2.2e-05|3.1e-05|2.3e-05
TNebmin	|1.1e-02	|9.6e-04|2.7e-03|4.2e-03|5.4e-03
MNebmax	|1.0e-09	|8.3e-11|2.8e-10|2.8e-10|2.8e-10
Wsmax10	|1.7e-02	|1.9e-02|1.8e-02|2.9e-02|1.7e-02
Wsmin10	|5.4e-03	|3.8e-03|5.9e-03|5.3e-03|4.6e-03
Wgmax	|2.4e-02	  |1.5e-02|1.5e-02|7.6e-03|1.5e-02
temp.demain	|2.7e-03|8.0e-04|2.9e-03|3.0e-03|3.4e-03


Je choisis donc le modèle $pluie7$ comme modèle de référence. Je les chercherai à l'améliorer ultérieurement (par le biais des interactions par exemple).


Je décide de vérifier les qualités explicatives du modèle $pluie7$ en le comparant au modèle $M_0$ et $M_{sat}$.


### D. Tests du rapport de vraissemblance


Il y a deux modèles extrêmes auxquels comparer $pluie7$ (en l'occurence $M_0$ et $M_{sat}$).

Pour rappel, on appelle  :

 * $M_0$ le modèlele nul dans lequel les $(Y_i)$ sont indépendantes identiquement distribuées, ce qui équivaut à $\beta_1 = \beta_2 = ... = \beta_k =0$.

 * $M_{sat}$ le modèlele saturé dans lequel il n’y a aucune structure aux ($p_i$).

Les trois modèles sont imbriqués ($M_0 \subset pluie7 \subset M_{sat}$).
On peut donc effectuer des **tests du $\chi^2$ de rapport de vraisemblance**.


```{r}
# modèle sans covariable
pchisq(summary(pluie7)$null.deviance-summary(pluie7)$deviance,
summary(pluie7)$df[1],lower = F)

# modèle saturé
pchisq(summary(pluie7)$deviance,summary(pluie7)$df[2],lower = F)
```


Dans le premier cas, on obtient une p-valeur très faible : je rejette le modèle sans covariable. Notre modèle est donc utile.

Dans le second cas, la p-valeur est également faible. Notre modèle ne capture pas toute la variabilité des données. je devrai normalement rejeter ce modèle au profit du modèle saturé, néanmoins je décide de le conserver pour l'instant.


### E. Qualité des modèles proposés en termes de déviance

Je décide de comparer l'ensemble de mes modèles aux deux modèles extrêmes ($M_0$ et $M_{sat}$). Cela est rendu possible par le fait qu'ils sont tous emboités avec $M_0$ et $M_{sat}$.


```{r}
mod2=c(paste('pluie',0:4,sep=''),"pluie4.1",paste('pluie',5:7,sep=''))

test_devi=function(model){

  # modèle sans covariable
  A=pchisq(summary(model)$null.deviance-summary(pluie7)$deviance,   summary(model)$df[1],
lower = F)

  # modèle saturé
  B=pchisq(summary(model)$deviance,summary(model)$df[2], lower = F)
  return(c(A,B))
}

df7=NULL ; df7a=NULL ; df7b=NULL
for (i in (1:length(mod2))){
  df7b=t(c(df7b,test_devi(get(mod2[i]))))
  df7a=(c(df7a,AIC(get(mod2[i]))))
}
df7b=matrix(df7b,ncol=2,nrow=9, byrow = T)
rownames(df7b)=mod2[1:length(mod2)]
df7=cbind(df7b,df7a)
colnames(df7)=c("vs mod  nul","vs modèle sat.","AIC")
kable(df7)
```


Aucun des modèles proposés captent l'ensemble de la variablité des données. En effet, le test de déviance montre quelque soit le modèle que $M_{sat}$ apporte de l'information supplémentaire (la p-valeur n'est significative).

Je décide de regarder à tout hasard les modèles proposés par les méthodes automatiques.


### F. Recherche automatique du meilleur modèle (sans interaction)

Je vérifie si les méthodes automatiques implémentées dans R ne fournissent pas un  "meilleur concurrent" en termes d'AIC.


Voici les codes respectifs pour les méthodes ascendantes, descendantes et Stepwise.


```{r}
modselect_f=stepAIC(pluie0,pluie.demain~.,data=
met3,trace=FALSE,direction=c("forward"))

#modèle sélectionné
reg_for=glm(formula = pluie.demain ~ Year + Tmoy2 + Tmin2 + Prmoy + Prmin + 
    Prmax + Plmoy + Snow + TNebmoy + TNebmin + MNebmax + LNebmax + 
    Wsmin10 + Wsmax10 + Wsmoy80 + Wdmoy80 + Wdmoy900 + Wgmax + 
    temp.demain, family = binomial, data = met3)

test_devi(reg_for)
```



```{r,include=FALSE}
modselect_f=stepAIC(pluie0,pluie.demain~.,data=
met3,trace=FALSE,direction=c("backward"))

# modèle sélectionné
reg_back=glm(formula = pluie.demain ~ Year + Tmoy2 + Tmin2 + Prmoy + Prmin + 
    Prmax + Plmoy + Snow + TNebmoy + TNebmin + MNebmax + LNebmax + 
    Wsmin10 + Wsmax10 + Wsmoy80 + Wdmoy80 + Wdmoy900 + Wgmax + 
    temp.demain, family = binomial, data = met3)

test_devi(reg_back)
```

```{r,include=FALSE}
modselect_f=stepAIC(pluie0,pluie.demain~.,data=
met3,trace=TRUE,direction=c("both"))

#modèle sélectionné
reg_both=glm(formula = pluie.demain ~ Year + Tmoy2 + Tmin2 + Prmoy + Prmin + 
    Prmax + Plmoy + Snow + TNebmoy + TNebmin + MNebmax + LNebmax + 
    Wsmin10 + Wsmax10 + Wsmoy80 + Wdmoy80 + Wdmoy900 + Wgmax + 
    temp.demain, family = binomial, data = met3)
summary(reg_both)
test_devi(reg_both)
```


Les 3 méthodes fournissent le même modèle. Ce modèle semble meilleur que $pluie7$ au sens du test de la déviance (comparé au modèle $M_{sat}$), puisque la $p-valeur$ est supérieure à $0.05$.
Néanmoins ce modèle utilise 20 covariables, dont un certain nombre n'est pas signicatif !


Au vu de ces résultats, je décide de procéder autrement. J'estime en effet que le volet prédictif de la variable à expliquer est plus important ici que dans le chapitre précédent.



## IV. La pluie (prédiction et amélioration du modèle)


Je décide donc de modifier mon plan d'étude (comparativement au chapitre précédent) en étudiant dès à présent les qualités prédictives du dernier modèle retenu (en l'occurence $pluie7$) sur le jeu de données "test".

Une fois cela accompli, je comparerai les qualités prédictives du modèle de référence à celui obtenu par les méthodes de recherche automatisées.

Je chercherai enfin à améliorer le modèle retenu en étudiant les interactions entre les covariables.
 

Mon modèle ultime sera celui qui optimisera simultanément le couple $prédictivité\ /\ explication$ tout en minimisant le nombre de covariables.


### A. Prédiction pour le modèle $pluie7$


```{r}

metpr1=metpr0[,-c(1,5,6)]
metpr2=metpr1

names(metpr2)=equivalences[,2]

#je réarrange les variables pour regrouper celles concernant la même catégorie
metpr2 <- metpr2[,c(1:4,23,22,5,25,24,6,27,26,7:9,29,28,10,31,30,11,33,32,12,35,34,13:15,37,36,16,17
,39,38,18,19,41,40,20,21,43,42,44,45)]
```


Je calcule les probabilités des valeurs prédites avec notre modèle retenu $pluie7$ 
("type=respponse" permettant de passer de l'échelle des "log côtes anglaises" à celle d'une probabilté). J'ai masqué les écarts-type pour plus de lisibilté.



```{r}
pred=function(model){
  A=predict(model, metpr2, type="response", se.fit=F)
  return(A)
}
head(pred(pluie7))
```


Je détermine la matrice de confusion associée à un seuil $s$ fixé (par ex. $s=0.6$) permettant de de classer les prédictions selon leurs type d'erreur (ou non) pour le modèle "pluie7".
Je choisis de fixer un poids identique à l'erreur de prédiction qu'elle soit de sensibilité ou de spécificité.


```{r}
  s=0.6
  # matrice de confusion
  print(table(pred(pluie7)>=s, metpr2$pluie.demain))
  # prédiction en %
  print(round(mean((pred(pluie7)>=s) == (metpr2$pluie.demain)),digits=4))
```

Ainsi pour $s=0.6$, on a environ $68,3 \%$ de bonnes prédictions.


Que se passe-t-il si je fais varier le seuil? Pour cela, je construis une table des bonnes prédictions.

```{r}
B=0
pred.tab = function(model,n,a,b){
  for (i in (0:n)) {
    A=round(seq(a,b,by=(b-a)/n),digits=9)
    length(B)=n+1
    B[i+1]=round(100*mean((pred(model)>=(a+(i*(b-a))/n)) == (metpr2$pluie.demain)),digits = 10)
    }
  C=data.frame(t(data.frame(A,B)))
  rownames(C)=c("seuil s","prédiction (en %)")
  colnames(C)=c()
return(C)
}

```


Ainsi, avec un pas de $0,1$ (soit $n=10$), j'obtiens le tableau suivant :

```{r}
pred.tab(pluie7,10,0,1)
```


Avec ce pas, un balayage du tableau indique que les meilleures prédictions sont fournies pour $s=0.5$ et les la fiabilité associée est de $71 \%$.

Je cherche à améliorer la précision du couple "sensibilité/spécificité" en affinant le pas (toujours par la méthode de balayage).



```{r}
#system.time(pred.tab(pluie7,20000,0,1))
```


La fonction "system.time" montre qu'une précision à $10^{-4}$ du maximum de prédiction prend plus d'une minute.


Le temps de calcul dans le tableau est un peu long et la recherche du seuil adéquat dans le tableau s'avère peu aisée.   
Pour y remédier, je crée une fonction qui après avoir "balayé le tableau" :

 * sélectionne la prédiction maximale $p$ et le seuil $s$ dans le tableau correspondant (d'amplitude $[a,b]$ et de pas $(b-a)/n$) 
 
 * affiche la matrice de confusion associée.
 

```{r}
pred_mat=function(model,n,a,b){

  # selection dans le tableau
  
  p=max(pred.tab(model,n,a,b)[2,])
  s=a+(which.max(pred.tab(model,n,a,b)[2,]))*(b-a)/n
  
  # matrice de confusion
  print(table(pred(model)>=s, metpr2$pluie.demain))
  
  return(c("s"=s,"prédic. (en %)"=p))
}

pred_mat(pluie7,20,0.41,0.43)
```


Le seuil $s=0,42$ permet d'améliorer les qualités prédictives du modèle, puisque la fiabilité s'élève désormais à $72.07\%$.


Effectuons des courbe ROC afin de visualiser les pouvoirs prédictifs selon les faux positifs (ou faux négatifs)


```{r, fig.width=9, fig.height=4.5}
library(ROCR)
par(mfrow=c(1,2))
p=prediction(pred(pluie7),metpr2$pluie.demain)
plot(performance(p,"tpr","fpr"))
abline(0,1)
abline(1,-1)

plot(performance(p,"tnr","fnr"))
abline(0,1)
abline(1,-1)
performance(p,"auc")@y.values[[1]]
```


l'AUC est de $0.758$. Etant supérieure à $0.7$, on peut la considérer comme bonne.

### B. Qualité prédictive des modèles automatiques (sans interaction)

Les méthodes descendantes et Stepwise fournissent un modèle utilisant les 20 covaraiables suivantes (dont un certain nombre ne sont pas signicatives) :

$Year, Tmoy2, Tmin2, Prmoy, Prmin, Prmax, Plmoy, Snow, TNebmoy, TNebmin, MNebmax, LNebmax, Wsmin10,
Wsmax10, Wsmoy80, Wdmoy80, Wdmoy900, Wgmax, temp.demain$

Je teste les qualités prédictives du modèle sur le jeu test.

```{r}
pred_mat(reg_both,100,0,1)
pred_mat(reg_both,100,0.4,0.5)
```


Le meilleur modèle proposé par les méthodes de sélection automatique est à mon sens moins bon que notre modèle $pluie7$. En effet, bien que l'AIC soit un peu inférieure, 

 * Le nombre de covariables présente est trop important.
 * la qualité de la prédiction est moins bonne (70.690 %)
 
--> Je conserve donc le modèle $pluie7$ comme base de travail.


### C. Amélioration du modèle de référence


Je pars du modèle $pluie7$. Il possède l'avantage d'avoir toutes ses covariables significatives, mais ne capte pas toute la variabilité des données.
Pour rappel, le modèle c'est une régression logistique contenant les covariables suivantes :

$Year, Tmoy2, Prmoy, Wsmoy80, Wdmoy80, Wdmoy900, Prmax, Prmin, TNebmin, MNebmax, Wsmax10, Wsmin10, Wgmax, temp.demain$


##### 1. Vue d'ensemble - Scattreplot

```{r}

# Scatterplot

# préparation des données

met5 <- met3[,c(2,5,10,11,12,16,23,31,32,33,36,37,43,45)]

pairs(met5,diag.panel=panel.hist,cex.labels=2,font.labels = 2,
      upper.panel=panel.cor, lower.panel=panel.lm)
```


Le nombre important de covariables entraine une difficulté de lecture des résultats :

 * bien que certaines covariables soient fortement corrélées, elles ne le sont pas avec toutes les autres (il est donc délicat de les enlever du modèle avec ce simple critère)
 * Certaines régressions semblent indiquer des essais possibles d'interactions exotiques du type $"y:\sqrt{x}"$ (mais l'interprétation s'avèrerait assez délicate)

Je décide donc de procéder autrement.


##### 2. Ajout des intercations

Il parait possible que que la température soit liée à la pression atmosphérique.
En effet, les fortes pressions sont généralement signes de temps non pluvieux et à contrario...

```{r}
cor.test(met3$Prmax,met3$Tmoy2)
```

La correlation est forte (p-valeur très faible), il peut être intéressant d'en étudier l'interaction dans le choix d'un modèle idéal. Par ailleurs, je remarque que, dans ce modèle, la corrélation est négative (information confirmée par le fait que l'intervalle de confiance ne contienne pas 0).
Cela laisse penser que la température et la pression maximale tendent à évoluer en "variations opposées" (ce qui tendrait à contredire mon intuition précédente)


```{r}
pluie7b=glm(pluie.demain~ -1 + Year+ Tmoy2*Prmax + Prmoy +Wsmoy80 +Wdmoy80 +Wdmoy900 +Prmin +TNebmin +MNebmax +Wsmax10 +Wsmin10 +Wgmax+temp.demain,
             ,family = binomial, data=met3)
```


L'AIC est meilleure ! (1259 au lieu de 1284), mais la covaraiable "Year" n'est désormais plus du tout significative (la p-valeur très élevée).
Je l'enlève du modèle.


```{r}
pluie7c=glm(pluie.demain~ -1 + Tmoy2*Prmax + Prmoy +Wsmoy80 +Wdmoy80 +Wdmoy900 +Prmin +TNebmin +MNebmax +Wsmax10 +Wsmin10 +Wgmax+temp.demain,
             ,family = binomial, data=met3)
```

Deux covariables sont à limite du seuil de significativité. Je décide de les conserver (temporairement) en attendant de voir si d'autres interactions pourraient "pencher les seuils du bon côté".

J'essaie d'augmenter la précision de la qualité de l'approximation de la prédiction en augmentant le pas du balayage. 


```{r}
pred_mat(pluie7c,10,0,1)
pred_mat(pluie7c,100,0,1)
pred_mat(pluie7c,20,0.365,0.367)
```

La prédiction est meilleure ! (p=73,1 % pour une seuil s=0,365), le tout pour un AIC à 1257.

Par ailleurs, le temps de calcul est désormais quasi immédiat. 

```{r}
#system.time(pred_mat(pluie7c,20,0.365,0.367))
```



J'ôte les covariables "un peu limite" en terme de significativité (Vdmoy80, Wsmin10, Wgmax) et en ne conservant que l'interaction entre les 2 dernières.



```{r}
pluie7d=glm(pluie.demain~ -1 + Tmoy2*Prmax + Prmoy +Wsmoy80 +Wdmoy900 +Prmin +Tmoy2:Wdmoy80 +TNebmin +MNebmax +Wsmax10 +Wsmin10:Wgmax+temp.demain,
             ,family = binomial, data=met3)
```

J'obtiens un AIC de 1253 et toutes les variables sont désormais significatives.


```{r}
# matrice de confusion (selon précision) et prédiction
pred_mat(pluie7d,100,0,1)
pred_mat(pluie7d,200,0.40,0.42)

# calcul de l'AUC
print("AUC=")
performance(prediction(pred(pluie7d),metpr2$pluie.demain),"auc")@y.values[[1]]

```

Je ne progresse pas sur la qualité de la prédiction par rapport au modèle précédent, mais j'ai gagné un peu en AIC.
De plus, l'AUC peut être considérée comme bonne (supérieure à 0.7).

J'étudie maintenant une interaction potentielle entre $temp.demain$ et $Prmax$.


```{r}
pluie7e=glm(pluie.demain~ -1 + (Tmoy2 + temp.demain)*Prmax + Prmoy +Wsmoy80 +Wdmoy900 +Prmin +Tmoy2:Wdmoy80 +TNebmin+MNebmax +Wsmax10 +Wsmin10:Wgmax,
             ,family = binomial, data=met3)
```


Je gagne un peu en AIC, mais ..


```{r}
pred_mat(pluie7e,100,0,1)
pred_mat(pluie7e,10,0.42,0.44)
```

j'ai ajouté une covariable au modèle et je perds en qualité prédictive.


Je décide de tester diverses interactions en observant la significativité de ces dernières tout en cherchant à minimiser l'AIC des modèles.

Je récapitulerai l'ensemble de ces résultats dans un tableau.


```{r}
pluie7f=glm(pluie.demain~ -1 + (Tmoy2 + temp.demain)*Prmax +temp.demain*Wdmoy900+ Prmoy +Wsmoy80 +Wdmoy900 +Prmin +Tmoy2:Wdmoy80 +TNebmin+MNebmax +Wsmax10 +Wsmin10:Wgmax,
             ,family = binomial, data=met3)

pred_mat(pluie7f,20,0.41,0.43)

#

pluie7g=glm(pluie.demain~ -1 + (Tmoy2 + temp.demain)*Prmax +temp.demain*(Wdmoy900+ 0)+ Prmoy +Wsmoy80 +Wdmoy900 +Prmin +Tmoy2:Wdmoy80 +TNebmin+ temp.demain:MNebmax +Wsmax10 +Wsmin10:Wgmax,
             ,family = binomial, data=met3)

pred_mat(pluie7g,100,0.38,0.4)

#

pluie7h=glm(pluie.demain~ -1 + (Tmoy2 + temp.demain)*Prmax +temp.demain*(Wdmoy900 + Wsmax10)+ Prmoy +Wsmoy80 +Wdmoy900 +Prmin +TNebmin+ temp.demain:MNebmax +Wsmax10  +temp.demain*Wsmax10 +Wsmin10:Wgmax,
             ,family = binomial, data=met3)

pred_mat(pluie7h,100,0.33,0.36)

#

pluie7i=glm(pluie.demain~ -1 + (Tmoy2 + temp.demain)*Prmax +Prmax:sqrt(Wgmax) +temp.demain*(Wdmoy900 + Wsmax10)+ Prmoy +Wsmoy80 +Wdmoy900 +Prmin +TNebmin+ temp.demain:MNebmax +Wsmax10  +temp.demain*Wsmax10 +Wsmin10:Wgmax,
             ,family = binomial, data=met3)
# summary(pluie7i)

pred_mat(pluie7i,100,0.3,0.5)
```

Chaque amélioration supplémentaire fait baisser un peu l'AIC (je n'ai pas indiqué les tests non concluants), mais la qualité prédictive ne s'améliore pas et le nombre de covariables intégrées s'incrémente de 1 à chaque nouvel essai, compliquant ainsi le modèle et son interprétation.

Aiguillé par le scatterplot, j'effetue un dernier essai en ajoutant l'interaction  $Prmax:sqrt(Wgmax)$.

L'AIC baisse encore d'un point (1231), mais le pouvoir prédictif n'évolue pas contrairement au nombre de covariables et à la complexité d'interprétation.

Je décide d'arrêter mes essais ici.


### D. Récapitulatif

J'ai choisi $pluie7$ comme modèle de base, puis j'ai cherché à étudier diverses interactions entre les covaraiables explicatives afin d'en minimiser l'AIC tout en maximisant le pouvoir prédictif.

Pour rappel, le modèle $pluie7$ est une régression logistique contenant les covariables suivantes :

$Year, Tmoy2, Prmoy, Wsmoy80, Wdmoy80, Wdmoy900, Prmax, Prmin, TNebmin, MNebmax, Wsmax10, Wsmin10,
Wgmax, temp.demain$
           
Voici un tableau résumant les divers résultats obtenus.

modèles | caractéristsiques du modèle | nbre de cov. | AIC | s | qualité de la prédiction / (sur 290)
--------| ----------------------------| ---- | ----| --| ------------------- |
pluie7  |aucune interaction                       | 14| 1284 | 0.422 | 72.069%  (209)
pluie7b |(pluie7) + Tmoy2*Prmax                   | 15| 1259 | 0.367 | 73.103% (212)
pluie7c |(pluie7b) - Year                         | 14| 1257 | 0.366 | 73.103% (212)
pluie7d |(pluie7c) - (Vdmoy80+Wsmin10+Wgmax)+Wsmin10:Wgmax| 12| 1253| 0.365 | 73.103% (212)
pluie7e | (pluie7d) + temp.demain*Prmax                | 13 | 1248 | 0.413 | 72.759% (211)
pluie7f | (pluie7e) + temp.demain*Wdmoy900               | 14 | 1245 | 0.413 | 73.103% (212)
pluie7g | (pluie7f) + temp.demain:MNebmax - MNebmax      | 15 | 1233 | 0.382 | 72.414% (210)
pluie7h |(pluie7g) + temp.demain*WSmax10 - Tmoy2:Wdmoy80 | 15 | 1232 | 0.339 | 71.724% (208)
pluie7i |(pluie7h) + Prmax:sqrt(Wgmax)                   | 16 | 1231 | 0.348 | 71.724% (208)



Le meilleur compromis entre :

 * nombre minimal de variables explicatives
 * AIC la plus basse
 * pouvoir prédictif le plus élevé
 
m'enclint à choisir le modèle $pluie7d$ (bien que l'AIC ne soit pas la plus faible).
En effet, ce modèle possède un pouvoir prédictif maximal et un nombre de covariable restreint (12 covariables).
Mon 2nd choix se serait porté sur le $pluie7f$ (même pouvoir prédictif, AIC meilleure de 2% mais 2 covariables supplémentaires).

Voici l'export des prédictions associées au modèle $pluie7d$.

```{r}
# Export des prédictions associées au modèle "temp4c2"

pluie.demain.pred=cbind(pred(pluie7d)>=0.40110,metpr2[44])
colnames(pluie.demain.pred)=c("pred.", "observ.")
head(pluie.demain.pred)
write.table(pluie.demain.pred, "pluie.demain.pred.csv", row.names=FALSE, sep=",")
```

### E. Modèles obtenus avec d'autres fonctions de lien

```{r}
pluie7db=glm(pluie.demain~ -1 + Tmoy2*Prmax + Prmoy +Wsmoy80 +Wdmoy900 +Prmin +Tmoy2:Wdmoy80 +TNebmin +MNebmax +Wsmax10 +Wsmin10:Wgmax+temp.demain,
             ,family = binomial(link=probit), data=met3)
AIC(pluie7db)

pluie7dc=glm(pluie.demain~ -1 + Tmoy2*Prmax + Prmoy +Wsmoy80 +Wdmoy900 +Prmin +Tmoy2:Wdmoy80 +TNebmin +MNebmax +Wsmax10 +Wsmin10:Wgmax+temp.demain,
             ,family = binomial(link=cauchit), data=met3)
AIC(pluie7dc)

pluie7dd=glm(pluie.demain~ -1 + Tmoy2*Prmax + Prmoy +Wsmoy80 +Wdmoy900 +Prmin +Tmoy2:Wdmoy80 +TNebmin +MNebmax +Wsmax10 +Wsmin10:Wgmax+temp.demain,
             ,family = binomial(link=cloglog), data=met3)
AIC(pluie7dd)
```

Les fonctions de lien utilisant "probit", "cauchit" et "cloglog" donne des AIC supérieures à celles de "logit" (1254, 1258 et 1257 respectivement), je conserve donc la fonction de lien "logit".




## V. Mesures prédictives des qualités prédictives (par validation croisée)

Je propose d'évaluer les qualités prédictives du modèle $pluie7d$ en effectuant une validation croisée de type "l-folds".

Pour cela, je divise les données $met3$ en $l$ parties. Chacune (tour à tour) des $l$ parties servant  de jeu "test" alors que le reste du jeu est dévolue au jeu "train". Chaque modèle testé sur l'ensemble des $l$ parties fournit $l$ qualités de prévision (construites avec des jeux de données restreints utilisant une fois et une seule l'ensemble des données initiales).

Je modélise le taux de prédiction théorique du modèle en répétant la procédure précédente $k$ fois. Ce taux est tout simplement la moyenne des $kl$ résultats précédents.

J'effectue ensuite la même procédure en subsituant le jeu "test" au profit de "metpr2" (le jeu de données servant initialement à tester le modèle prédictif).


Mon objectif est de vérifier que le taux de bonnes prédictions de $73.1\%$ est assez proche "du mieux que le modèle retenu peut proposer".

Plusieurs remarques :

 * Le choix de $l$ peut s'avérer délicat (suffisament grand pour "apprendre" et pas trop grand pour éviter le surapprentissage). L'usage veut que l'on choisisse une valeur $l$ vérifiant $5\leq l\leq15$. Je choisis $l=10$
 
 * Le fait d'apprendre sur des modèles restreints devrait en théorie minimiser la qualité de prédiction 
 
 * je pourrais faire cette validation sur l'ensemble des modèles proposés précédemment. La principale contrainte serait le temps de calcul..
 

Je reprends une partie des fonctions précédentes. Je les modifie légèrement pour les besoins de mes tests.

```{r}
# fonction de prédiction
pred2=function(model,test){
  A=predict(model,test , type="response", se.fit=F)
  return(A)
}
# table de prédiction
B=0
pred.tab2 = function(model,n,a,b,data_test,test2){
  for (i in (0:n)) {
    A=round(seq(a,b,by=(b-a)/n),digits=9)
    length(B)=n+1
    B[i+1]=round(100*mean((pred2(model,data_test)>=(a+(i*(b-a))/n)) == (test2)),digits = 10)
    }
  C=data.frame(t(data.frame(A,B)))
  rownames(C)=c("seuil s","prédiction (en %)")
  colnames(C)=c()
return(C)
}

# prédiction maximale et seuil associé
pred_mat2=function(model,n,a,b,data_test,test2){

  # selection dans le tableau
  
  p=max(pred.tab2(model,n,a,b,data_test,test2)[2,])
  s=a+(which.max(pred.tab2(model,n,a,b,data_test,test2)[2,]))*(b-a)/n

  return(c("s"=s,"prédic. (en %)"=p))
}
```




Voici ma fonction effectuant ma validation croisée en découpant mon jeu de données en $l$ parties.


```{r}


val_crois =function(l,n,a,b,jeu_pred){
  B=NULL
  C=NULL
  D=NULL
  m=nrow(met3)
  for (i in 1:l){
    k = sample(m,(l-1)/l*m)
    data_train = met3[k,]
    data_test = met3[-k,]
    
    #utilisation des covariables intervenant dans le modèle "temp4d"
    
    reg=glm(pluie.demain~ -1 + (Tmoy2 + temp.demain)*Prmax +temp.demain*Wdmoy900+ Prmoy +Wsmoy80 +Wdmoy900 +Prmin +Tmoy2:Wdmoy80 +TNebmin+MNebmax +Wsmax10 +Wsmin10:Wgmax,
             family = binomial, data=data_train)


  C[i]=pred_mat2(reg,n,a,b,jeu_pred,jeu_pred$pluie.demain)
  D[i]=pred_mat2(reg,n,a,b,jeu_pred,jeu_pred$pluie.demain)[2]

  }
  df9=t(data.frame(C,D))
  rownames(df9)=c("s","p")
  return(df9)
}

kable(val_crois(3,400,0,1,metpr2))
```




Cette fonction détermine l'erreur moyenne de prévision après simulation de $k$ répétitions.

```{r}

prev_moy=function(l,n,a,b,jeu_pred,k){
  E=NULL
  F=NULL
  
  for (i in 1:k){
    E=val_crois(l,n,a,b,jeu_pred)[2,]
    F=c(E,F)
  }
  F=F[-l*k-1]
  F=mean(F)
return(F)
}
```



voici un récapitulatif après 2, 3 et 5 essais.


```{r}
df10=data.frame(c(prev_moy(10,100,0,1,met3,2),prev_moy(10,100,0,1,met3,3),prev_moy(10,100,0,1,met3,5)),
c(prev_moy(10,100,0,1,metpr2,2),prev_moy(10,100,0,1,metpr2,3),prev_moy(10,100,0,1,metpr2,5)))
rownames(df10)=c("2 répétitions","3 répétitions","5 répétitions")
colnames(df10)=c("met3","metpr2")
kable(df10)

```

Je décide d'effectuer des simulations de plus grand effectif

```{r, eval=FALSE}
prev_moy(10,100,0,1,met3,10)
prev_moy(10,100,0,1,metpr2,10)
prev_moy(10,100,0,1,met3,100)
prev_moy(10,100,0,1,metpr2,100)
prev_moy(10,100,0,1,met3,500)
prev_moy(10,100,0,1,metpr2,500)
```


Voici les taux de prédiction en $\%$ (obtenus auprès d'un certain temps pour les simulations comprenant un grand nombre d'essais)

nombre d'essais | $met3$ | $metpr2$ 
--------| ---| ---
10  | 74.8| 72.1
100 | 74.8| 72.1
500 | 74.8| 72.1

Je constate que le jeu de test $met3$ minimise l'erreur moyenne de prédiction. Cela ne semble pas illogique puisque le jeu "test" à servi à construire le modèle. L'adéquation aux données est donc meilleure qu'avec le jeu de données $metpr2$.
 
Au vu des résultats obtenus sur le jeu d'entrainenement, je pense être assez proche de la vérité.

En effet, le jeu "test" $met3$ fournit une qualité prédictive meilleure d'envrion $3.7\%$, mais il sous-évalue l'erreur moyenne de prédiction pour les raisons ennoncées ci-dessus. 



## VI. Conclusion

Pour chacune de ces deux variables d’intérêt, j'ai proposé et validé un modèle  ($temp4c2$ pour la température et $pluie7d$ pour la pluie). 
J'ai tenté de **sélectionner un modèle optimal** au sens de la dualité  $explicatif \ / \ prédictif$.

J'ai toutefois privilégié le cadre prédictif pour la 2nde variable d'intérêt, car il m'est apparu moins important de de ne pas me tromper de plus de 1°C sur la température du lendemain que de savoir si je devais ou non "prendre mon parapluie demain..." 


