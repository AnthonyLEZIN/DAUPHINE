---
title: "Projet ML - Clustering"
author: "Anthony LEZIN"
date: "4/19/2021"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# I - Généralités et préparation des données
\vspace{0,2cm}
```{r,echo=FALSE, message =F, warning = F}
library(knitr)
library(tidyverse)
library(MASS)
library(ROCR)
library(here)
library(caret)
library(readr)
library(ggplot2)
library(RColorBrewer)
library(dendextend)
library(GGally)
library(cluster)
library(mclust)
library(here)
```
\vspace{0,2cm}

Commençons par charger les données

\vspace{0,2cm}

```{r}
# Chargement groupé

files_names <- list.files(path=here("./Clustering/project-5-files"))

nb_files <- length(files_names)
data_names <- vector("list",length=nb_files)
data_names <- strsplit(files_names, split=".csv")

for (i in 1:nb_files) {
      assign(data_names[[i]],
                read.csv(paste(here("./Clustering/project-5-files", files_names[i])))
     )}
```

\vspace{0,2cm}

Le but de l'analyse exploratoire est de caractériser les départements français du point de vue de leur composition en population générale et en population active. L'analyse doit être mise en oeuvre en deux étapes.

**La première étape** devrait consister à créer deux ensembles de données agrégées au niveau du département.
Dans ces ensembles de données, chaque département métropolitain français doit être décrit par des statistiques résumant la composition de la population.   
Par exemple, on pourrait décrire un département par le pourcentage de ses habitants féminins, son nombre d'habitants, la répartition des diplômes des habitants, des salaires, etc. Il est recommandé de n'inclure des variables numériques que dans les descriptions pour faciliter l'analyse . Le premier ensemble de données doit correspondre aux départements tels que caractérisés par leur population générale, tandis que dans le second ensemble de données, les départements doivent être décrits en utilisant la population active. Le rapport doit contenir une description des statistiques utilisées pour représenter les départements.

**Dans un deuxième temps**, les deux ensembles de données doivent être analysés à l'aide d'algorithmes de clustering et de visualisation.  
Le but de l'analyse est d'évaluer si les départements français sont homogènes en termes de composition de la population ou, au contraire, séparés en différents groupes.
Vous pouvez utiliser n'importe quel algorithme de clustering vu pendant le cours, puis évaluer la composition du cluster et le positionnement relatif en utilisant une cartographie linéaire et / ou non linéaire, des tests statistiques basés sur une analyse de variance, etc. très important. Finalement, vous pouvez envisager d'illustrer si les clusters résultants sont spatialement corrélés, à l'aide d'une cartographie géographique.

\vspace{0,2cm}

Dabs cette première partie, je créer deux ensembles de données agrégées au niveau du département.

\vspace{0,2cm}
```{r,message =F}
# Jointure de city
city=city_loc %>%
    left_join(city_adm) %>%
    left_join(city_pop) %>% 
    left_join(departments) %>% 
    relocate(Nom.de.la.commune, Dep,Nom.du.département, REG,inhabitants)%>%
    relocate(insee_code,TOWN_TYPE, .after = Long)
head(city)
```

\vspace{0,2cm}

-----------------------------------------------------------------------------------------------------------

\vspace{0,2cm}

1 -EDU1 -Pas de scolarité ou arrêt avant la fin du primaire  
2 -EDU2 -Aucun diplôme et scolarité interrompue à la fin du primaire ou avant la fin du collège  
3-EDU3 -Aucun diplôme et scolarité jusqu’à la fin du collège ou au-delà  
4 - EDU11 -CEP (certificat d’études primaires)  
5 - EDU12 - BEPC, brevet élémentaire, brevet des collèges, DNB  
6 - EDU13 -CAP, BEP ou diplôme de niveau équivalent  
7 - EDU14 - Baccalauréat général ou technologique, brevet supérieur, capacité en droit, DAEU, ESEU  
8 - EDU15 - Baccalauréat professionnel, brevet professionnel, de technicien ou d’enseignement,   
9 - EDU16 - BTS, DUT, Deug, Deust, diplôme de la santé ou du social de niveau bac+2, diplôme équivalent  
10 -EDU17 - Licence, licence pro, maîtrise, diplôme équivalent de niveau bac+3 ou bac+4  
11 - EDU18 - Master, DEA, DESS, diplôme grande école niveau bac+5, doctorat de santé  
12 - EDU19 - Doctorat de recherche (hors santé)  

\vspace{0,2cm}

Je créée une variable supplémentaire "edu" permettant de numériser le niveau d'étude de la façon suivante :

 * ses valeurs représentent le nombre approximatif d'années d'études à partir de l'année de CP (année 0).
 * lorsque ce nombre n'est pas renseigné où lorsqu'il s'étend sur une plage de plusieurs années, la valeur est fixée au nombre d'année médian (par exemple 2.5 est attribué au nombre d'année d'un individu ayant arrêté sa scolarité entre le CP et le CM2 non révolu)   
 * Lorsqu'une examen diplomant achève l'année, une majoration de 0.1 est attribué. Ainsi, pour un individu ayant terminé son cycle scolaire en terminale, son nombre d'année d'étude est 12.1 (5 années de primaire, 4 de collège, 3 de lycée et 0.1 du dipôme)

\vspace{0,2cm}

```{r, message=F}
# Jointure de toutes les data "learn" afin de former un fichier "train" avec les données "city"
train_cl=learn %>%
      left_join(learn_job) %>%  
      left_join(city)

train_cl=mutate(train_cl,
             edu=case_when
              (train_cl$highest_degree=="EDU1"~"2.5",
                train_cl$highest_degree=="EDU2"~"5",
                train_cl$highest_degree=="EDU3"~"8",
                train_cl$highest_degree=="EDU11"~"5.1",
                train_cl$highest_degree=="EDU12"~"8.1",
                train_cl$highest_degree=="EDU13"~"10.1",
                train_cl$highest_degree=="EDU14"~"12.1",
                train_cl$highest_degree=="EDU15"~"12.1",
                train_cl$highest_degree=="EDU16"~"14.1",
                train_cl$highest_degree=="EDU17"~"15.6",
                train_cl$highest_degree=="EDU18"~"17.1",
                train_cl$highest_degree=="EDU19"~"20.1"
              )
            ) %>% 
      dplyr::select(Person_id, highest_degree,edu, SEX, insee_code, Dep, wage, inhabitants, X, Y)

# Ajout de 0 pour la variable manquantes "wage"
train_cl$wage[is.na(train_cl$wage)]<-0
train_cl$edu=as.numeric(train_cl$edu)
train_cl[10005:10009,]
```

\vspace{0,2cm}

Enfin, je résume chaque département par des statistiques résumant la composition de la population telles  : 

 * le pourcentage de ses habitants féminins
 * son nombre d'habitants
 * la répartition des diplômes des habitants
 * le salaire moyen
 * le niveau d'étude 
 



```{r, message=F}

# levels(train_cl$highest_degree)
# années_edu=c("2.5","5.1","8.1","10.1","12.1","12.1","14.1","15.6","17.1","20.1","5","8")

# 
# train_cl$edu=
#   for (i in 1:nrow(train_cl)){
#     if(train_cl$highest_degree==levels(train_cl$highest_degree)[i]){
#       train_cl$edu=années_edu[i]
#     }
#   }
# 
#    for (i in 1:nrow(train_cl)){
#     levels(train_cl$highest_degree)[i]
#       train_cl$edu=années_edu[i]
#     }

```

\vspace{0,2cm}


```{r, message=F}
# Une fonction regroupant les infomrations importante par département
table_function=function(model){
  salaire_moyen=model %>% group_by(Dep) %>% 
    summarize(salaire_moyen=mean(wage))
  
  années_étude=model %>% group_by(Dep) %>% 
    summarize(années_étude=mean(edu))

  indiv_par_dep=model %>% group_by(Dep) %>% 
  summarize(nb_individus=n())

  Femmes_par_dep=model %>% group_by(Dep) %>% 
    summarize(nb_femmes=sum(SEX=="Female")) 

  infos_par_dep=model %>%
    group_by(insee_code,Dep) %>% 
    summarize(nb_villes=n(),nb_habitants=mean(inhabitants))%>% 
    group_by(Dep) %>% 
    summarize(nb_villes=n(),nb_habitants=sum(nb_habitants))

  temp_model_cl=infos_par_dep %>% 
    left_join(salaire_moyen) %>% 
    left_join(indiv_par_dep) %>% 
    left_join(Femmes_par_dep) %>% 
    left_join(années_étude) %>% 
    mutate(taux_femmes=100*nb_femmes/nb_individus) %>% 
    dplyr::select(-nb_femmes) %>% 
    relocate(Dep,nb_habitants,nb_individus,salaire_moyen,années_étude,taux_femmes)
  
return(temp_model_cl)
}
```

\vspace{0,2cm}

Je crée donc 2 jeux de données :  

 * un premier ensemble de données correspondant aux départements tels que caractérisés par leur population générale  
 * un second ensemble de données où seule la population active est prise en compte  

\vspace{0,2cm}

```{r, message=F}
train_cl_positif=filter(train_cl,wage>0)
```

\vspace{0,2cm}
```{r, message=F}
train_cl_actif=table_function(train_cl_positif)
train_cl_actif

train_cl_full=table_function(train_cl)
train_cl_full
```

\vspace{0,2cm}

J'effectue une carte descriptive des indicateurs que je trouve intéressant.  
Le nombre pertinent de catégories est l'objet des parties ultérieures.

Pour l'exemple, je visualise des cartes avec des variables comportant 5 ou 9 catégories.
\vspace{0,2cm}

```{r,message=FALSE,warning=F}
library(GADMTools)
France <- gadm_sf_loadCountries("FRA", level=2 )

train_cl_actif2=train_cl_actif %>% left_join(departments) 
train_cl_actif2=rename(train_cl_actif2,NAME_2=Nom.du.département)
mydata <- data.frame(train_cl_actif2)
head(mydata)
```

\vspace{0,2cm}
```{r,message=FALSE}
    choropleth(France, 
               data = mydata, 
               step=5,
               value = "taux_femmes", 
               adm.join = "NAME_2",
               breaks = "quantile", 
               palette = "YlOrRd",
               legend="Taux de femmes par départements",
               title="représentation du taux de femmes par départements") 
```


```{r,message=FALSE}
    France_par_salaire=choropleth(France, 
               data = mydata, 
               step=5,
               value = "salaire_moyen", 
               adm.join = "NAME_2",
               breaks = "quantile", 
               palette = "Blues",
               legend="Salaire moyen par départements",
               title="représentation des salaires par départements") 
 France_par_salaire
```

\vspace{0,2cm}
```{r,message=FALSE}
    choropleth(France, 
               data = mydata, 
               step=5,
               value = "années_étude", 
               adm.join = "NAME_2",
               breaks = "quantile", 
               palette = "Greens",
               legend="Nombre d'année d'études par départements",
               title="représentation du nombre d'année d'études par départements") 
```

\vspace{0,2cm}
Il semble y avoir une corrélation entre le nombre moyen d'année d'étude et le salaire moyen.

\vspace{0,2cm}

```{r, message=F}
quant_9=quantile(as.numeric(train_cl_actif$salaire_moyen), probs=seq(0, 1, 1/9))

list_quant_9=vector("list",9)
  for (i in 1:9){
    list_quant_9[[i]]=
      which(quant_9[i]<=as.numeric(train_cl_actif$salaire_moyen) & 
      as.numeric(train_cl_actif$salaire_moyen)<quant_9[i+1])
  }
list_quant_9
```


\vspace{0,2cm}

# II. Classification hiérarchique classique

\vspace{0,2cm}

décrire un département par le pourcentage de ses habitants féminins, son nombre d'habitants, la répartition des diplômes des habitants, des salaires, etc. Il est recommandé de n'inclure des variables numériques que dans les descriptions pour faciliter l'analyse.

\vspace{0,2cm}
```{r, warning=F}
train_cl_sans_dep=train_cl_full %>% 
  dplyr::select(-Dep)

dend_cl1 <- hclust(dist(train_cl_sans_dep, method="euclidean"), method="single")
plot(dend_cl1)
plot(dend_cl1$height, type="b")
```
\vspace{0,2cm}
La règle du coude engagerait à choisir 5, 9, voire 12 clusters.  

\vspace{0,2cm}
```{r, warning=F}
clusters <- cutree(dend_cl1, 9 ,order_clusters_as_data = F)
summary(as.factor(clusters))
dend_cl1_col_9<- color_branches(as.dendrogram(dend_cl1), clusters=clusters,
                       col=1:9, size=0.2)
plot(dend_cl1_col_9, leaflab = "none", yaxt="none")
```
\vspace{0,2cm}
La décomposition dans chacune des configurations est excrécable.  
Par exemple, avec 9 clusters, 72 départements ne semblent former qu'un seul cluster et 5 clusters ne contiennent qu'un seul département...  

Cette méthode ne semble pas adaptée pour un clustering adéquat.
\vspace{0,2cm}

# III. Clustering hiérarchique avec la méthode de Ward

\vspace{0,2cm}

```{r, message=F}
dend_cl_ward <- hclust(dist(train_cl_sans_dep, method="euclidean"), method="ward")
plot(dend_cl_ward)
plot(dend_cl_ward$height, type="b")
inertie <- sort(dend_cl_ward$height, decreasing = TRUE)
plot(inertie[1:20], type = "s", xlab = "Nombre de classes", ylab = "Inertie")
points(c(2, 5, 9), inertie[c(2, 5, 9)], col = c("green3", "red3", "blue3"), cex = 1.5, lwd = 2)
```
\vspace{0,2cm}

L'étude de l'inertie par rapport aux nombres de classes semblent confirmer les choix raisonnable de 5 et/ou 9 clusters  ($k=2$ est clairement trop petit et $k=12$ me semble trop élevé).

Créons une fonction permettant de visualiser le dendogramme, ainsi que la répatrtition de la décomposition
\vspace{0,2cm}

```{r, warning=F}
clusters_ward=function(n)
  { 
    tmp<- cutree(dend_cl_ward,n,order_clusters_as_data = F)
    dend_cl1_ward_n<- color_branches(as.dendrogram(dend_cl_ward), clusters=tmp,col=1:n)
  
    plot(dend_cl1_ward_n, leaflab = "none", yaxt="none")
    return(summary(as.factor(tmp)))

  }
```

\vspace{0,2cm}

```{r, warning=F}
clusters_ward(5)
clusters_ward(9)
```

C'est beaucoup mieux avec la méthode de Ward. Les "sauts" provoqués par le nombre de clusters semblent équilibrer la décomposition. 

\vspace{0,2cm}

```{r, message=F}
# Dois-je garder cette partie ?


plot.list_cl1 <-
  list(ggplot(as.ggdend(dend_cl1_col)),
       ggplot(train_cl_sans_dep, aes(salaire_moyen,nb_individus)) +
         geom_point(col=clusters, size=0.5))

ggmatrix(plot.list_cl1, nrow=1, ncol=2, showXAxisPlotLabels = F,
         showYAxisPlotLabels = F, xAxisLabels=c("dendrogram", "scatter plot")) +
         theme_bw()

#table(clusters, train_cl_full$Dep)
```

\newpage

# IV. Clustering avec K-means 

\vspace{0,2cm}
On émet l'hypothèse que les données ont une distribution gaussienne avec des variances isotropes (sans direction privilégiée). Sous cette hypothèse, la méthode des k-means peut se révéler intéressante.  
Compte-tenu des résultats précédemment obtenus, j'effectue un k-means avec 5 ou 9 clusters et une initialisation aléatoire.

```{r}
# K-means avec 5 classes
kmeans_res.train_cl.pp_5 <- LICORS::kmeanspp(data=train_cl_sans_dep, k = 5, 
                         start = "random", iter.max = 100, nstart = 1)
```

\vspace{0,2cm}

```{r}
kmeans_res.train_cl.pp_5
```

\vspace{0,2cm}
On observe une répartition un peu différente de celle de de Ward.

\vspace{0,2cm}

```{r}
summary(as.factor(kmeans_res.train_cl.pp_5$cluster))
```

\vspace{0,2cm}

```{r}
train_cl_full.aov=aov(kmeans_res.train_cl.pp_5$cluster~.,data=train_cl_sans_dep)
summary(train_cl_full.aov)
```

\vspace{0,2cm}

Une analyse de la variance fournit les résultats suivants :  

3 covariables semblent le plus disciminer (les p_values sont très faibles) :  
le nombre d'habitants, le salaire moyen, et le nombre d'années d'études
(pour rappel, le nombre d'individus représente le nombre d'individus étudié par départements)

```{r}
list_means=vector("list",5)
for (i in 1:5){
  list_means[[i]]=which(kmeans_res.train_cl.pp_5$cluster==i)
}
list_means
#table(res.train_cl.pp$cluster,train_cl_full$Dep)
```

\vspace{0,2cm}

```{r}
kmeans_res.train_cl.pp=function(n)
  {
    tmp=LICORS::kmeanspp(data=train_cl_sans_dep, k = n,start = "random", iter.max = 100, nstart = 1)
    print(tmp)
    return(summary(aov(tmp$cluster~.,data=train_cl_sans_dep)))
  }
```

\vspace{0,2cm}

```{r}
kmeans_res.train_cl.pp(5)
```

\vspace{0,2cm}

```{r}
kmeans_res.train_cl.pp(9)
```

\vspace{0,2cm}

Que ce soit en classification à 5 ou 9 clusters, le salaire moyen reste une covariable très discriminante. En revanche, le nombre d'années d'études n'est pas discriminant à 5 clusters contrairement à 9.

### Qualité du clustering pour k-means

\vspace{0,2cm}

Le k-means produit des classes parfaitement sphériques. Le nombre de clusters a été déterminé par le biiais d'une autre classification.  
Utilisons des indicateurs pour déterminer la valeur optimale de $k$.

```{r}
kmeans.res.indices <- matrix(0, nrow=15, ncol=3)
for (i in 1:15)
{
  #K-means
  #kmeans.res <- LICORS::kmeanspp(blobs[,1:2], k=i, start="random")
  kmeans.res<-LICORS::kmeanspp(data=train_cl_sans_dep, k = i, 
                         start = "random")
  # calcul de la silhouette de chaque observation
  s.data.set <- silhouette(kmeans.res$cluster, dist(train_cl_sans_dep, method="euclidean"))
  # % de variance expliquée
  kmeans.res.indices[i,1] <- kmeans.res$betweenss/kmeans.res$totss
  # calcul de la silhouette moyenne
  kmeans.res.indices[i,2] <- (kmeans.res$betweenss/(i-1))/(kmeans.res$tot.withinss/(dim(train_cl_sans_dep)[1]-i))
  if (is.null(dim(s.data.set))==T)
    kmeans.res.indices[i,3] <- NA
  else
    kmeans.res.indices[i,3] <- mean(s.data.set[,3])
}
kmeans.res.indices <- as.data.frame(kmeans.res.indices)
```

\vspace{0,2cm}

```{r}
plotList <- list(ggplot(data=kmeans.res.indices, aes(x=1:15, y=kmeans.res.indices[,1])) + 
                   geom_bar(stat="identity", fill="steelblue") +
      geom_text(aes(label=round(kmeans.res.indices[,1],2)), vjust=1.6, color="white", size=3.5)+
                   labs(y="Explained variance variance") +
                   scale_x_continuous(breaks=1:15) + theme_bw(),
                 ggplot(data=kmeans.res.indices, aes(x=1:15, y=kmeans.res.indices[,2])) + 
                   geom_bar(stat="identity", fill="steelblue") +
                   geom_text(aes(label=round(kmeans.res.indices[,2],2)), vjust=1.6, color="white", size=3.5)+
                   labs(y="Calinski-Harabasz Index") +
                   scale_x_continuous(breaks=1:15) + theme_bw(),
                 ggplot(data=kmeans.res.indices, aes(x=1:15, y=kmeans.res.indices[,3])) + 
                   geom_bar(stat="identity", fill="steelblue") +
                   geom_text(aes(label=round(kmeans.res.indices[,3],2)), vjust=1.6, color="white", size=3.5)+
                   labs(y="Average silhouette") +
                   scale_x_continuous(breaks=1:15) + theme_bw())

pm <- ggmatrix(
  plotList, nrow=3, ncol=1, showXAxisPlotLabels = F, showYAxisPlotLabels =F ) + theme_bw()
pm
```

\vspace{0,2cm}

Les indicateurs sont loin d'être d'accord et leur lecture s'avère peu aisée.

Il semblerait que 4, voire 5 clusters soit une bonne valeur pour le pourcentage de variance expliquée.  
L'indice silhouette fournit 2 clusters (ce qui semble trop peu au vu de la nature des données) et un maximal local à 8 clusters
Enfin, l'indice de Calinski-Harabasz propose, quant à lui, 12 clusters, (ce qui me semble beaucoup).

\vspace{0,2cm}
```{r}

```

\vspace{0,2cm}

```{r}

```

De plus, le kmeans produit un clustering crisp (une observation est affectée à un et un seul cluster)
Le k-means est très intéressant pour faire de la réduction de dimensions.
Le k-means permet de "résumer "travailler" les données en petits clusters que l'on résume par le centre des classes

\newpage

# V. Clustering avec GGM

```{r,message=FALSE}
```


```{r,message=FALSE}

```


```{r, message=F}

```


On va chercher à combiner k-means et classification hiérarchique,

Le k-means est très intéressant pour faire de la réduction de dimensions.
Le k-means permet de "résumer "travailler" les données en petits clusters que l'on résume par le centre des classes

```{r, message=F}

```

\vspace{0,2cm}

# V. Modèles bayésiens (Expectation-Maximization algorithm)

\vspace{0,2cm}

On va chercher à utiliser un algorithme de mélange de modèle appliqué au clustering (Gaussian Mixture Mode).  
L'idée principale est que ,par rapport aux autres modèles type k-means, une classe ne possède pas l'exclusivité sur un individu, mais plutôt une probabilité d'appartenance. Ce point important se retrouve également à l'étape de mise à jour des  paramètres ("step M") où la moyenne pondérée par les probabilités d'appartenance se retrouve "plus lissée" que dans les autres modèles précédemment

\vspace{0,2cm}

```{r, message=F}
# EM avec le vrai nombre de classes et sans contraintes sur la variance
res.train_cl.em_9 <- Mclust(train_cl_sans_dep, G=9, verbose = T)
```

```{r, message=F}

names(res.train_cl.em_9)
class(res.train_cl.em_9)
```

\vspace{0,2cm}

```{r, message=F}
# Paramètres estimés par EM
summary(res.train_cl.em, parameters = T)
```

\vspace{0,2cm}

```{r, message=F}
#res.train_cl.em$classification
#table(res.train_cl.em$classification)
la_liste=list()
for (i in 1:length(res.train_cl.em$classification))
  {
    la_liste[[i]]=(which(res.train_cl.em$classification==i))
}
la_liste=la_liste[1:9]
la_liste
```

\vspace{0,2cm}

```{r,message=FALSE}
res.train_cl.em <- Mclust(train_cl_sans_dep, G=2:12, verbose = T)
```

\vspace{0,2cm}

```{r,message=FALSE}
T1<-Sys.time()
res.train_cl.em <- Mclust(train_cl_sans_dep, G=2:12, verbose = T)
T2<-Sys.time()
T2-T1
```

\vspace{0,2cm}

```{r,message=FALSE}
summary(res.train_cl.em$BIC)
plot(res.train_cl.em$BIC,ylim=c(-8200,-7200))
```

\vspace{0,2cm}

Selon le critère de pénalisation BIC, les 3 modèles les plus performants sont :

 * VVV à 2 clusters (ellipsoidal, varying volume, shape, and orientation)
 * VVE à 5 et 6 clusters (ellipsoidal, equal orientation)

\vspace{0,2cm}

```{r,message=FALSE}
round(res.train_cl.em$BIC,digits=2)
```

\vspace{0,2cm}
je sélectionne le modèle à 6 clusters.

\vspace{0,2cm}
```{r,message=FALSE}
res.train_cl.em_best <- Mclust(train_cl_sans_dep, G=6,modelNames="VVE",verbose = T)
```

\vspace{0,2cm}

\vspace{0,2cm}
```{r,message=FALSE}
summary(res.train_cl.em_best)
```

\vspace{0,2cm}
```{r, message=F}
#res.train_cl.em$classification
#table(res.train_cl.em$classification)
la_liste=list()
for (i in 1:length(res.train_cl.em_best$classification))
  {
    la_liste[[i]]=(which(res.train_cl.em_best$classification==i))
}
la_liste=la_liste[1:6]
la_liste
```

\vspace{0,2cm}

```{r,message=FALSE}

```


```{r, message=F}

```


```{r, message=F}

```








----------------------------------------------------------------------------------------------------------

```{r, message=F}
train_cl_actif_sans_dep=train_cl_actif %>% 
  dplyr::select(-Dep)

dend_cl_actif <- hclust(dist(train_cl_sans_dep, method="euclidean"), method="ward")
plot(dend_cl_actif)
plot(dend_cl_actif$height, type="b")

###########################################################################
clusters_actif <- cutree(dend_cl_actif, 9 ,order_clusters_as_data = F)
summary(as.factor(clusters_actif))
dend_cl_actif_col<- color_branches(as.dendrogram(dend_cl_actif), clusters=clusters_actif,
                       col=1:9)
plot(dend_cl_actif_col, leaflab = "none", yaxt="none")
```



# IV. Clustering mixte (k-means + classification hiérarchique)


## B. Premiers essais

\vspace{0,2cm}

```{r}
library("FactoMineR")
library("factoextra")

```

```{r}
#FAMD(train_full[sample(1:100000,40),], ncp = 5, sup.var = NULL, ind = NULL, graph = TRUE)
```



\vspace{0,2cm}
```{r}

```
\vspace{0,2cm}


\vspace{0,2cm}

```{r,message=FALSE}

```
\vspace{0,2cm}
```{r}
```



\vspace{0,2cm}

```{r,message=FALSE}

```


\vspace{0,2cm}
```{r,message=FALSE}

```


\vspace{0,2cm}
